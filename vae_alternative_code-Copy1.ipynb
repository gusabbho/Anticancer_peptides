{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a1c59-e213-45b8-9673-370a70a9c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, Dense, Lambda, Layer\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77689dde-5bdd-4980-89bc-caf9ca3a1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = pd.read_excel(\"acp.fasta.xlsx\")\n",
    "acp['seq_Length'] = acp['sequence'].apply(lambda x: len(x))\n",
    "acp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3cf51-5b34-4fc8-a0e2-b8d4cd0c82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_acp= pd.read_excel(\"non-acp.fasta.xlsx\")\n",
    "non_acp['seq_Length'] = non_acp['sequence'].apply(lambda x: len(x))\n",
    "non_acp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a99c5-8126-41c5-92ae-2ad94c40db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_seq= pd.concat([acp,non_acp])\n",
    "whole_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e8730-be6f-4da8-9537-d8a716dcfe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_seq['sequence'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f219b9-6a4c-4ff2-85f3-a1462a86e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_seq = whole_seq.drop_duplicates(subset='sequence', keep='first')\n",
    "whole_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cf44e-4c84-4ab9-bf77-48296735d556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69b4a2-d741-4fbe-9216-4626fc1fecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Amino acid encoding\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "# Encode sequences\n",
    "all_sequences = whole_seq['sequence'].tolist()\n",
    "\n",
    "# Filter sequences shorter than 85 amino acids\n",
    "sequences = [seq for seq in all_sequences if len(seq) <= 85]\n",
    "# Create indices for splitting\n",
    "indices = np.arange(len(sequences))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.03,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Further split train_indices into train_indices and validation_indices\n",
    "train_indices, validation_indices = train_test_split(\n",
    "    train_indices,\n",
    "    test_size=0.1,  # Adjust the validation set size as needed\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split sequences based on indices\n",
    "training_sequences = [sequences[i] for i in train_indices]\n",
    "validation_sequences = [sequences[i] for i in validation_indices]\n",
    "testing_sequences = [sequences[i] for i in test_indices]\n",
    "\n",
    "# Find the maximum sequence length across all sets\n",
    "max_len = max(max(len(seq) for seq in training_sequences),\n",
    "              max(len(seq) for seq in validation_sequences),\n",
    "              max(len(seq) for seq in testing_sequences))\n",
    "\n",
    "# Function to pad sequences and create masks\n",
    "def process_sequences(sequences):\n",
    "    encoded_seqs = [[aa_to_idx[aa] for aa in seq] for seq in sequences]\n",
    "    padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(encoded_seqs, padding='post', maxlen=max_len)\n",
    "    mask = tf.cast(padded_seqs != 0, dtype=tf.float32)  # 0 represents padding, 1 represents actual data\n",
    "    one_hot_seqs = tf.one_hot(padded_seqs, len(amino_acids))\n",
    "    return one_hot_seqs, mask\n",
    "\n",
    "# Process sequences for each set\n",
    "train_sequences, train_mask = process_sequences(training_sequences)\n",
    "valid_sequences, valid_mask = process_sequences(validation_sequences)\n",
    "test_sequences, test_mask = process_sequences(testing_sequences)\n",
    "\n",
    "# Print shapes of the resulting tensors\n",
    "print(\"Training sequences shape:\", train_sequences.shape)\n",
    "print(\"Validation sequences shape:\", valid_sequences.shape)\n",
    "print(\"Testing sequences shape:\", test_sequences.shape)\n",
    "print(\"Training mask shape:\", train_mask.shape)\n",
    "print(\"Validation mask shape:\", valid_mask.shape)\n",
    "print(\"Testing mask shape:\", test_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd67484-c1d8-4cf6-8397-490cfe0d25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_info():\n",
    "    cmd = 'nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv,noheader,nounits'\n",
    "    result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "    gpu_info = result.stdout.strip().split(', ')\n",
    "    return gpu_info\n",
    "\n",
    "gpu_utilization, gpu_memory_used = get_gpu_info()\n",
    "print(f\"GPU Utilization: {gpu_utilization}%\")\n",
    "print(f\"GPU Memory Used: {gpu_memory_used} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fde441-2b81-4010-81f3-550dbc2e2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Gradients do not exist for variables.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73805a-b987-4594-9f48-dbd9e41b9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't show warning every time so it might help prevent the jupyter from crashing.\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd2bf1-f561-4ef0-a910-5f692c337cf5",
   "metadata": {},
   "source": [
    "# VAE model with deeper GRU layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67141ea1-5afa-4f0b-b0f1-37e897e2e866",
   "metadata": {},
   "source": [
    "# Loss instaiblity\n",
    "\n",
    "Here are a few suggestions to improve your VAE model for generating peptide sequences:\n",
    "\n",
    "- Since most of your sequences are shorter than 85 amino acids, you could reduce the max sequence length (max_len) to something like 100 instead of 207. This will allow the model to better focus on the length distribution seen in the training data. (done)\n",
    "\n",
    "- Try using 1D convolutional layers before or after the GRU layers. Conv1D layers can help learn local patterns and motifs in the sequences. (done)\n",
    "\n",
    "- Add dropout layers after the GRU and dense layers to prevent overfitting. A dropout rate of 0.2-0.5 is commonly used.\n",
    "\n",
    "- Reduce the latent space dimension (latent_dim) to 32 or 48. This will regularize the model. You can then gradually increase it if needed. (done)\n",
    "\n",
    "- Use a recurrent layer like LSTM instead of GRU if you want to better capture long-range dependencies. \n",
    "\n",
    "- Try a shallower and wider architecture rather than deep and narrow. For example, use only 1-2 GRU layers with 256-512 units rather than 4 layers of 64 units.\n",
    "\n",
    "- Use batch normalization and layer normalization to stabilize training.\n",
    "\n",
    "- Use a learning rate scheduler like cosine decay to slowly reduce the LR over epochs.(done)\n",
    "\n",
    "- Use a larger batch size if possible to smooth out noise during training.\n",
    "\n",
    "The key is to experiment with different architectures, optimizers, regularizers and hyperparameters to find the right balance for your specific sequence generation task. Start simple and modify incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc04aa2-eb19-4332-907e-187ab502ce58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0bbf521-dea2-43aa-bbc5-a3e5e13334c0",
   "metadata": {},
   "source": [
    "The loss value you're observing in the training process (1.3822e-05) seems to be extremely low and doesn't make sense, especially if your validation loss (val_loss) is negative. This could be indicative of a training issue. A few things to check:\n",
    "\n",
    "- Scaling Issues: Make sure your input data is properly scaled. Extremely small values can lead to numerical instability and unexpected results in training.\n",
    "\n",
    "- Loss Function: Ensure that your loss function is properly defined and calculated. Check if the Wasserstein loss and any other loss components are being computed correctly.\n",
    "\n",
    "- Learning Rate: The learning rate and decay rate in your optimizer might need adjustments. A learning rate that's too high or too low can impact convergence.\n",
    "\n",
    "- Gradient Clipping: If your gradients are exploding, it can lead to unusual loss values. Consider adding gradient clipping to prevent this.\n",
    "\n",
    "- Batch Size: Smaller batch sizes can lead to faster convergence in some cases.\n",
    "\n",
    "- Regularization: Ensure you're not using excessive regularization, which could lead to overly small weight updates.\n",
    "\n",
    "- Data Preprocessing: Verify that your data preprocessing is consistent, especially with regard to padding and sequence lengths.\n",
    "\n",
    "- Debugging Outputs: Add additional print or logging statements to monitor the values of the loss components and other key variables during training.\n",
    "\n",
    "- Model Architecture: Sometimes, extreme reductions in loss can be indicative of a model that's too large or too powerful for the task. Consider simplifying your model architecture further.\n",
    "\n",
    "- Evaluation Metric: Ensure you're using appropriate evaluation metrics for your task. Negative validation loss might indicate a problem with the evaluation metric.\n",
    "\n",
    "- Given the complexity of the issue, it might be helpful to try several debugging steps and experiment with different approaches to narrow down the cause of the unusual loss behavior.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c221a7-c13d-49e2-9db4-5bc39c032b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6673c1-e288-40ef-910a-34b4ef799cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\"\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# VAE Model\n",
    "latent_dim = 32\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "  # Reparameterization trick\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(max_len,))\n",
    "x = layers.Embedding(len(amino_acids), latent_dim)(encoder_inputs)\n",
    "x = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Add bidirectional GRU with simpler units\n",
    "x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.GRU(128))(x)\n",
    "# Reshape the output for the next GRU layer\n",
    "x = layers.Reshape((-1, 256))(x)\n",
    "x = layers.GRU(256, kernel_regularizer=l2(0.01))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.BatchNormalization()(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean', kernel_regularizer=l2(0.01))(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var', kernel_regularizer=l2(0.01))(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, max_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.gru1 = layers.GRU(128, return_sequences=True)\n",
    "        self.gru2 = layers.GRU(128, return_sequences=True)\n",
    "        self.dense = layers.Dense(256, kernel_regularizer=l2(0.01))\n",
    "        self.output_layer = layers.Dense(len(amino_acids), activation='softmax', kernel_regularizer=l2(0.01))\n",
    "\n",
    "    def call(self, inputs, sequence_length):\n",
    "        reshaped_inputs = tf.expand_dims(inputs, axis=1)\n",
    "        repeated_inputs = tf.repeat(reshaped_inputs, sequence_length, axis=1)\n",
    "\n",
    "        x = self.gru1(repeated_inputs)\n",
    "        x = self.dense(x)\n",
    "        decoded_output = self.output_layer(x)\n",
    "\n",
    "        return decoded_output\n",
    "\n",
    "decoder = Decoder(latent_dim, max_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Update the VAE model to use the custom WassersteinLossLayer\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructions = self.decoder(z, sequence_length=max_len)\n",
    "        kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructions\n",
    "\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "\n",
    "# Reshape train and test sequences\n",
    "train_sequences = tf.reshape(train_sequences, (-1, max_len))\n",
    "test_sequences = tf.reshape(test_sequences, (-1, max_len))\n",
    "valid_sequences = tf.reshape(valid_sequences, (-1, max_len))\n",
    "\n",
    "\n",
    "\n",
    "# Define the learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Use Adagrad optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "vae.compile(optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss', patience=10, restore_best_weights=True\n",
    "# )\n",
    "\n",
    "\n",
    "history_vae = vae.fit(\n",
    "    train_sequences, train_sequences,\n",
    "    epochs=80,\n",
    "    batch_size = 32,\n",
    "    validation_data=(valid_sequences, valid_sequences),\n",
    "    verbose=1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cec088-9872-4b74-a398-661770fcc8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81448f-34f7-4201-8e3a-e587f8666d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access training and validation losses\n",
    "train_loss = history_vae.history['loss']\n",
    "# val_loss = history_vae.history['val_loss']\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1707a5-4847-472c-a393-f32ba214ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206f5f8-fb29-4c99-9ffe-6d9add76de71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc060023-015b-4165-94e1-fa03e150cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences_with_varying_lengths(vae_model, sequences, masks):\n",
    "    generated_sequences = []\n",
    "\n",
    "    for i, (seq, mask) in enumerate(zip(sequences, masks)):\n",
    "        seq_length = int(tf.reduce_sum(mask))  # Get the actual length of the sequence based on the mask\n",
    "        # Sample a latent vector from a normal distribution\n",
    "        sampled_z = np.random.normal(size=(1, latent_dim))\n",
    "\n",
    "        # Decode the sampled latent vector to generate a sequence with the actual length\n",
    "        generated_sequence_indices = vae_model.decoder(sampled_z, sequence_length=seq_length)\n",
    "        \n",
    "        # Convert generated sequence indices to amino acid letters\n",
    "        generated_sequence = ''\n",
    "        for idx_seq in generated_sequence_indices:\n",
    "            subsequence = [amino_acids[int(np.argmax(probs))] for probs in idx_seq]\n",
    "            generated_sequence += ''.join(subsequence)\n",
    "        \n",
    "        # Append the generated sequence to the list\n",
    "        generated_sequences.append(generated_sequence)\n",
    "\n",
    "    return generated_sequences\n",
    "\n",
    "# Generate sequences with varying lengths from the test set\n",
    "generated_sequences = generate_sequences_with_varying_lengths(vae, test_sequences, test_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84600e5-eb64-4827-9260-1efedec07ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "for s1, s2 in zip(generated_sequences, testing_sequences):\n",
    "    distance = Levenshtein.distance(s1, s2)\n",
    "    print(f\"Levenshtein distance between '{s1}' and '{s2}': {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f89d0-0f95-4dcc-ac7b-8e5ed701e36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
